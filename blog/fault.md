# Fault tolerance for distributed data systems is really quite simple

First since you are here, I think you might appreciate this short mnemonic I jotted down in my grad school distributed systems class: 
- **Optimistic concurrency**:  shotgun marriage, possible divorce
- **Two phase commit**: getting engaged before you marry
- **Three phase commit**: asking if your SO is going to say yes before the proposal

Fault tolerance is an exceedingly dry topic, perhaps surpassed only by statistical mechanics in its capacity to induce hair loss. In this post I will discuss both *batch* and *streaming* systems, like Trino (formerly known as Presto), Spark, Kafka, Flink, and Quokka. Now on to business. 

First, why bother with fault-tolerance? Batch systems and streaming systems have very different answers to this question. Batch systems like Spark typically tackle offline ETL jobs, and fault tolerance is needed to keep the retry costs down on long running jobs on spot instances. Real-time streaming systems like Flink perform real-time data processing, and fault tolerance is needed to make sure the service (e.g. Visa fraud detection) can get back up and running. These different objectives heavily impact the fault tolerance design, which typically involves some tradeoff between **how quickly you can recover** and **how much overhead you impose** in normal operation. 

Offline batch systems typically optimize for cost and don't have strict SLAs. Therefore, they typically optimize for lower overhead in normal operation. Real-time streaming systems care about not dying, so they naturally optimize for the former. Also note that the notion of "fault tolerance" here is intrinsically tied to computation within a single data processing job, not the durability of the underlying data sources. For most of these systems, we assume the data sources are persistent and infinitely replayable (S3 or Kakfa topics).

Now that we know **what** these systems are trying to achieve, let's take a look at **how** they achieve it. Fault tolerance is typically done with three techniques, **spooling**, **checkpointing** and **lineage**. Let's define them one by one. It's highly recommended to read them in order.

- **Spooling**: all the aforementioned data systems consist of workers passing messages to each other. If we think that one of the workers could go up in flames, it might seem prudent to keep a copy of all the messages you send him. The copy could be reliable (i.e. Kafka topic, HDFS or S3) or unreliable (i.e. memory of the message sender). Kafka, Trino and the good ole MapReduce take the former approach while Spark and Quokka take the second. Reliable spooling can cause severe overhead in normal operation since persisting things in distributed applications like this inevitably means replication and thus network IO. This is particularly bad if network IO is already your main bottleneck in normal operation (think TB-scale Spark shuffles).

- **Lineage**: when a worker goes up in flames, we should figure out what objects were lost on that worker and remake them. In case that worker had some internal state, we should recreate that too. This requires the system to track what input objects are used to create each object, aka lineage. It really helps to assign unique names to objects when doing this, and it really helps if all the objects are **immutable** to maintain a consistent name -> object mapping. The lineage could be determined statically, as in Spark, or durably logged during execution, as in Quokka. 

	Since this lineage information is usually very small compared to actual data in batch processing systems, keeping track of this data structure, even dynamically, imposes negligible overhead. In streaming systems when each object can consist of only one or two records, this is usually not feasible. In these cases you can have a static *strategy* to dynamically fix the lineage, like processing records in order of event time. Then you just have to follow the same strategy upon recovery.
	
	Assuming we had spooled the worker's input objects, armed with lineage, we can just spin up a clone of the dead worker and replay all of its inputs. If some of the inputs are also lost (unreliable copy), then you have to figure out how to remake those as well from their own lineage.
	
- **Checkpointing**: lineage and spooling are all that's required for functional fault recovery. However, especially in real-time systems which could be up and running for months on end, reconstructing a dead worker by replaying *all* its historical input might be impractical. Indeed in such scenarios, storing all its historical inputs is also impractical. To reduce recovery times and save storage space, we can simply store a reliable checkpoint of all the objects on the worker and its state at a point in time. Then all the inputs which arrived at the worker before this point in time can be safely discarded, as their effects have been persisted. 
	
	Flink allows users to create a consistent snapshot checkpoint of their entire streaming application through the Chandy-Lamport algorithm. In fact I believe this is the key for its popularity over Kafka Streams, as Kafka Streams users keep complaining about long recovery times by replaying the state changelog. Spark also allows users to checkpoint an entire RDD in applications with many stages, so that buffered RDDs prior to the checkpoint can be discarded, which they call *lineage truncation*.

	Similar to reliable spooling, checkpointing can impose huge overhead in normal operation, particularly if your application is already network-IO bound. In addition, coordinated checkpointing schemes like Flink's can result in network spikes when everybody decides to checkpoint at once, a common problem in large-scale deployments.

OK. I'm done. **See, I promised -- fault tolerance in distributed data systems are really quite simple**. Both streaming and batch systems more or less use the same tools to achieve their different tradeoffs in normal operation overhead and recovery performance. Let's now take a look at individual systems. *If you are lazy just read the last sentence of each paragraph.*

- **Spark**: workers do not have state and only manage immutable data objects. The lineage between those objects are determined by the coordinator statically before the application starts. By default, shuffle data messages are spooled unreliably on the sender, and no reliable spooling or checkpointing is used. Upon failure, lineage is consulted to recreate lost objects that are still needed by future objects by replaying their inputs. If the inputs are also lost, lineage is again consulted to reconstruct them. Lost objects can be reconstructed in parallel -- so even if you end up doing a lot of recovery work, you can do said work in parallel. *Spark optimizes for low overhead in normal operation for potentially more work upon recovery*. 
- **Kafka Streams**: all messages between workers are persisted reliably in Kafka topics. Lineage is not tracked as execution order is deterministic based on event time. AFAIK, checkpointing is not a first class citizen, so fault tolerance relies on replaying the input Kafka topics, which can take hours in large applications. *In a sense, Kafka Stream's design sets it up for great fault recovery performance, if it just adds checkpointing support! Perhaps that's why everybody's using Flink now.*
- **Trino**: Trino added fault tolerance support via Project Tardigrade last year. Shuffle messages are persisted to HDFS or cloud storage and lineage is statically determined like Spark. This is the Kafka Streams philosophy applied to a Spark-like system. *This design optimizes for fault recovery performance at the expense of normal operation*. 
- **Flink**. Flink is a bit interesting. It does not store any messages or lineage at all. However, as aforementioned, it performs periodic checkpointing. Flink's fault tolerance relies on **not committing any output unless the state that produced it has been checkpointed**. Upon recovery, Flink will only start reading inputs not processed by the last checkpoint, making sure it never regenerates any outputs that have been previously committed. This mechanism makes sense as in the absence of lineage, Flink could very well generate different outputs for the same input sources upon recovery past the last checkpoint, but that's okay since the outputs were not committed the first time around. *This design actually trades off normal operation performance for fast fault recovery, since it holds up committing outputs!*
- **Quokka**: Quokka resembles Spark. The most important difference is that lineage in Quokka is dynamically determined and logged. This is because Quokka is a push-based pipelined system. In such a system it's natural for a worker to decide for itself what next to process based on objects pushed to it. After the worker makes the decision on what inputs to take to make the next object, it has to reliably log this decision. Upon recovery, it is this log that must be consulted for correct fault recovery. Similar to Spark, Quokka can engage in parallel recovery. 

If you are still with me at this point and have not lost any hair, give yourself a pat on the back. If you are one of the maintainers of the systems I mentioned above and spot any factual errors, please raise a Github issue in the repo. If you are interested in learning more about distributed fault tolerance, here's the next record from my distributed systems class notes:
- **Paxos**:  who knows
